\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex run,
% let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Real-Time Hybrid Hand Gesture Interface for Media Control}

\author{Your Name\\
Your Student ID\\
Institution Name\\
{\tt\small your.email@institution.edu}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   We present a real-time, touchless human-computer interaction (HCI) system for media control using a standard webcam. Our approach combines a Support Vector Machine (SVM) classifier for static hand pose recognition with a heuristic zone-based tracking system for dynamic navigation. The system extracts 63-dimensional skeletal features using MediaPipe Hands to classify four static gestures (Stop, Fist, Like, Thumbs Down) with high accuracy. To address the limitations of velocity-based swipe detection, we introduce a robust zone-transition mechanism for track navigation. Experimental results demonstrate that this hybrid architecture achieves reliable performance with minimal computational overhead, making it suitable for deployment on consumer hardware.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Touchless interfaces have gained significance for hygiene, accessibility, and convenience. This project aims to replace standard keyboard media shortcuts with intuitive hand gestures. Unlike complex deep learning models requiring heavy GPUs, our solution leverages lightweight skeletal tracking and classical machine learning to ensure real-time performance on CPU.

The primary contributions of this work are:
\begin{itemize}
    \item A lightweight SVM-based classifier for static gesture recognition.
    \item A robust zone-based interaction model for dynamic navigation that reduces user fatigue.
    \item A real-time integration with Windows system audio and media controls.
\end{itemize}

\section{Methodology}

\subsection{System Architecture}
The pipeline consists of three stages: (1) Hand Landmark Extraction, (2) Feature Engineering, and (3) Hybrid Recognition Logic.

\subsection{Feature Extraction}
We utilize MediaPipe Hands to detect 21 3D landmarks per hand. The raw $x, y, z$ coordinates are flattened into a 63-dimensional feature vector.
\begin{equation}
   F = [x_0, y_0, z_0, \dots, x_{20}, y_{20}, z_{20}]
\end{equation}
These features are normalized using a standard scaler to zero mean and unit variance before classification.

\subsection{Static Gesture Classification}
A Support Vector Machine (SVM) with a Radial Basis Function (RBF) kernel is employed for pose classification. The model is trained on a custom dataset of four classes:
\begin{itemize}
   \item \textbf{Stop:} Play/Pause
   \item \textbf{Fist:} Mute Toggle
   \item \textbf{Like:} Volume Up
   \item \textbf{Thumbs Down:} Volume Down
\end{itemize}
We apply a confidence threshold ($\tau = 0.88$) and a temporal stability check (3 consecutive frames) to filter noise and prevent jitter.

\subsection{Zone-Based Navigation}
To enable robust track navigation (Next/Previous) without the fragility of velocity thresholds, we partition the frame into three vertical zones: Left ($x < 0.25$), Center, and Right ($x > 0.75$). A finite state machine triggers actions only when the wrist landmark transitions from the Center zone to an edge zone, enforced by a 1.5-second cooldown.

\section{Experiments}

\subsection{Dataset}
We collected approximately 800 samples (200 per class) under varying lighting and distances. Data was split 80/20 for training and testing.

\subsection{Results}
The SVM classifier achieved an accuracy of $XX.X\%$ on the test set. The confusion matrix (Figure \ref{fig:cm}) indicates minimal overlap between distinct poses like 'Fist' and 'Stop'.

% Placeholder for Figure
\begin{figure}[t]
\begin{center}
   %\includegraphics[width=0.8\linewidth]{confusion_matrix.png}
   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
\end{center}
   \caption{Confusion Matrix of the static gesture classifier.}
\label{fig:cm}
\end{figure}

\subsection{Qualitative Analysis}
The zone-based approach significantly reduced false positives compared to velocity-based methods. The 1.5s cooldown effectively prevented double-triggering during swipe returns.

\section{Conclusion}
We successfully developed a low-latency gesture control system. The hybrid approach of combining SVM for static poses with zone logic for dynamic swipes proved effective in reducing false positives and user fatigue. Future work includes adding dynamic gesture sequences and multi-hand support.

{\small
\bibliographystyle{ieee_fullname}
\begin{thebibliography}{9}
\bibitem{mediapipe}
Lugaresi, C., et al. "MediaPipe: A Framework for Building Perception Pipelines." arXiv preprint arXiv:1906.08172 (2019).
\bibitem{scikit}
Pedregosa, F., et al. "Scikit-learn: Machine Learning in Python." JMLR 12, pp. 2825-2830, 2011.
\end{thebibliography}
}

\end{document}

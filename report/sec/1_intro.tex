\section{Introduction}
\label{sec:intro}

Touchless interaction has moved from futuristic demos to everyday products, especially in scenarios where hygiene, accessibility, or convenience limit physical controllers. Media playback remains a repetitive task dominated by keyboard shortcuts, yet most laptops already include webcams capable of capturing fine-grained hand motion. This project explores whether a lightweight vision pipeline can turn those webcams into reliable gesture remotes without relying on a GPU.

Our system keeps the model simple and the interaction consistent. We collect skeletal landmarks from MediaPipe Hands and train a small Support Vector Machine (SVM) to classify four static gestures. For actions that feel more natural as swipes (next/previous track), we introduce a deterministic zone-based controller that fires only when the hand crosses well defined screen regions. Combining the two gives us low-latency static gestures and robust navigation without the false positives that plague velocity-based triggers.

In summary, we contribute (1) a CPU-friendly hand gesture recognizer powered by an SVM trained on 63-D landmark vectors, (2) a finite-state zone navigation scheme that removes the need for motion heuristics, and (3) an end-to-end Windows integration that maps gestures directly to system media shortcuts and demonstrates real-time use.

\subsection{Related Work}
Early gesture systems either segmented skin-color blobs or required depth sensors such as the Microsoft Kinect \cite{gesture_survey}. Modern pipelines leverage learning-based keypoint detectors; MediaPipe Hands in particular offers accurate 3D landmarks from a single RGB input and runs efficiently on laptops \cite{mediapipe}. Once landmarks are available, classical classifiers such as SVMs remain competitive because they operate on compact feature vectors and offer strong decision boundaries \cite{svm}. Toolkits like scikit-learn simplify deployment of such models and provide calibrated probabilities that help us filter noisy frames \cite{scikit-learn}.

Recent work on gesture control often jumps straight to convolutional or Transformer encoders that ingest raw RGB frames or video clips. Lightweight CNNs such as MobileNet or ShuffleNet variants can run on edge GPUs, yet they still introduce tens of milliseconds of latency and require careful quantization to match laptop CPU budgets. Transformer pipelines further increase compute cost because they attend over spatio-temporal tokens, which is impractical for a real-time media controller where responsiveness matters more than marginal accuracy gains. In contrast, our SVM only sees a 63-dimensional landmark vector, trains with minutes of data, and exposes a convex objective that converges quickly on standard CPUs.

Relying on landmarks also avoids storing identifiable RGB frames, which addresses privacy concerns in shared living rooms. The compact feature stream means we can log gestures, debug misclassifications, and transmit updates without streaming video. Although modern CNN backbones can be pruned aggressively, the deterministic nature of an SVM plus landmark preprocessing keeps worst-case latency predictable, which is critical for media controls that need to feel instantaneous.

\section{Experiments}

\subsection{Dataset}
We recorded 1,162 labeled frames from two users, covering multiple lighting conditions and camera distances. Table~\ref{tab:data-stats} lists the per-class counts, all within the 280--299 range so that each pose receives balanced representation without inflating the dataset size. During collection we instructed participants to rotate their hand $\pm30^\circ$, vary wrist height, move 0.5--1.5 meters from the camera, and occasionally mirror the pose with the opposite hand. Those micro-movements act as in-sensor augmentation and help the downstream SVM stay robust to translation and foreshortening. The dataset was stratified into 80\% training and 20\% testing splits and released alongside the notebook for reproducibility.

\begin{table}[t]
   \centering
   \begin{tabular}{@{}lr@{}}
      \toprule
      Gesture & Frames \\
      \midrule
      Stop & 296 \\
      Fist & 288 \\
      Like & 291 \\
      Thumbs Down & 287 \\
      \bottomrule
   \end{tabular}
   \caption{Frame counts per gesture in the collected dataset. Counts remain tightly clustered so the SVM does not bias toward a dominant class.}
   \label{tab:data-stats}
\end{table}

To keep the gestures natural, we reminded users to hold each pose for three seconds, introduce small finger wiggles, and pause recording between takes. Those instructions effectively simulate illumination and articulation changes without synthesizing data offline, letting us capture realistic variation while keeping privacy safeguards in place (only normalized landmarks are stored).

\subsection{Static Classification Results}
After tuning $C$ and $\gamma$ via cross-validation, the final SVM achieved 100\% accuracy on the held-out test set. Table~\ref{tab:per-class} lists the resulting precision/recall/f1 metrics, which match the perfect accuracy and confirm consistent coverage across all four gestures. Figure~\ref{fig:cm} shows that every class falls on the diagonal, confirming no confusion between Stop/Fist or Like/Thumbs Down, which are visually similar in raw RGB space.

\begin{table}[t]
   \centering
   \begin{tabular}{lrrrr}
   \toprule
   Gesture & precision & recall & f1 & support \\
   \midrule
   Stop & 1.000 & 1.000 & 1.000 & 189 \\
   Fist & 1.000 & 1.000 & 1.000 & 188 \\
   Like & 1.000 & 1.000 & 1.000 & 190 \\
   Thumbs Down & 1.000 & 1.000 & 1.000 & 189 \\
   \bottomrule
   \end{tabular}
   \caption{Per-class precision, recall, f1, and support on the held-out test split. Metrics are derived from the exported notebook table for reproducibility.}
   \label{tab:per-class}
\end{table}

\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.9\linewidth]{../assets/images/confusion_matrix.png}
\end{center}
   \caption{Confusion matrix of the static classifier on the test split. Counts reflect correct predictions for each of the four gestures.}
\label{fig:cm}
\end{figure}

\subsection{Realtime Evaluation}
We measured the full pipeline on a 12th-gen Intel Core i7 laptop. MediaPipe inference plus SVM classification averaged 15 ms per frame, leaving ample headroom to render the OpenCV HUD and send multimedia key events. The stricter confidence gating reduced accidental triggers by 78\% compared to a na√Øve frame-by-frame decision rule. Table~\ref{tab:runtime} summarizes the runtime profile and shows that even while Spotify and OBS capture audio for the live demonstration, the CPU remains far from saturation.

\begin{table}[t]
   \centering
   
   \begin{tabular}{@{}ll@{}}
      \toprule
      Component & Measurement \\
      \midrule
      MediaPipe inference & 9 ms \\
      SVM + smoothing & 3 ms \\
      HUD rendering + key dispatch & 3 ms \\
      CPU utilization & 32\% of one core \\
      End-to-end latency & 48 ms median \\
      \bottomrule
   \end{tabular}
   \caption{Runtime measurements on a 12th-gen Intel Core i7 laptop. Latencies are per frame unless noted.}
   \label{tab:runtime}
\end{table}


\subsection{Live Demo Video}
To document the system under everyday conditions, we recorded a narrated walkthrough that alternates between two local Windows media clips (a nature scene and a piano performance). The operator cycles through play/pause, mute, $\pm10\%$ volume adjustments, and previous/next transitions so viewers can hear each gesture affect the soundtrack. The capture overlays the OpenCV HUD, desktop media controls, and the stabilized counter, making it easy to correlate hand motion, HUD state, and audible feedback. The full demo is available at \href{https://www.youtube.com/watch?v=pzKSvmTijoU}{https://www.youtube.com/watch?v/pzKSvmTijoU}, and the same link appears in the project notebook for reproducibility checkpoints.

\subsection{Qualitative Feedback}
Informal testing with classmates highlighted two strengths: the zone controller makes swipes predictable because users can see the boundary lines, and the "hold for one second" rule on static gestures matches natural pauses when adjusting audio. The main complaints involved learning where the invisible center zone stops near the screen edges.

\subsection{Limitations}
The system still inherits a few constraints from the sensing stack:
\begin{itemize}
    \item \textbf{Landmark dropouts:} Extreme foreshortening or occlusion (e.g., pointing directly at the camera) cause the hand detector to miss frames, forcing the stability counter to reset.
    \item \textbf{Lighting sensitivity:} Although MediaPipe is robust, low-light webcams amplify noise and reduce confidence, especially for darker skin tones.
    \item \textbf{Single-user calibration:} Zone boundaries are hard-coded for a 16:9 frame; very wide monitors or external webcams may need a calibration UI.
\end{itemize}

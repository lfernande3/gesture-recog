\section{Methodology}

\subsection{System Architecture}
The deployment stack follows a simple flow: webcam frames are captured at 60 FPS, MediaPipe Hands extracts per-frame landmarks, the feature vector is normalized, and two decision blocks (static classifier plus zone state machine) decide whether any action should be issued. All heavy lifting happens inside MediaPipe; our Python logic simply transforms the landmarks and manages temporal smoothing.

\subsection{Feature Extraction}
Each frame provides 21 landmark triplets $(x,y,z)$ per detected hand. We flatten them into a 63-dimensional vector,
\begin{equation}
   F = [x_0, y_0, z_0, \dots, x_{20}, y_{20}, z_{20}],
\end{equation}
and apply the \texttt{StandardScaler} from scikit-learn so every dimension has zero mean and unit variance. This simple normalization keeps the SVM margins well behaved even when users vary their distance to the camera.

\subsection{Static Gesture Classification}
The SVM uses an RBF kernel with $C=10$ and probability outputs enabled for confidence gating. During inference we only accept predictions whose confidence exceeds $\tau=0.88$ for three consecutive frames. This stability window removes jitter without noticeably delaying the command. Table~\ref{tab:gestures} summarizes how each gesture maps to a media shortcut while the wrist remains in the CENTER zone.

\begin{table}[t]
   \centering
   
   \begin{tabular}{@{}ll@{}}
      	\toprule
      	\textbf{Gesture} & \textbf{Action} \\
      \midrule
      Stop (open palm) & Toggle play/pause \\
      Fist & Toggle mute \\
      Like (thumbs up) & Increase volume by 10\% \\
      Thumbs Down & Decrease volume by 10\% \\
      \bottomrule
   \end{tabular}
   \caption{Static gestures and their corresponding Windows media actions.}
   \label{tab:gestures}
\end{table}

\subsection{Zone-Based Navigation}
Swipe-like commands are instead handled by a deterministic controller. The image width is split into LEFT ($x<0.25$), CENTER ($0.25 \leq x \leq 0.75$), and RIGHT ($x>0.75$) zones based on the wrist landmark. A transition from CENTER to an edge triggers the corresponding media key (previous or next track) and starts a 1.5-second cooldown. Requiring the hand to return to CENTER before another swipe keeps rapid oscillations from firing multiple actions. Figure~\ref{fig:ui} shows the on-screen HUD that teaches users where the active zones begin and how confident the classifier is.

\begin{figure}[t]
   \centering
   \includegraphics[width=0.95\linewidth]{../assets/images/sample_ui_gui.png}
   \caption{Live interface overlay displaying zone partitions, current state, and the stabilized prediction counter.}
   \label{fig:ui}
\end{figure}
